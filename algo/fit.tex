\documentclass[aps,12pt]{revtex4}
\usepackage[a4paper]{geometry}
\usepackage{graphicx}
\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage{bm}
\usepackage{pslatex}
\usepackage{chemarr}
\usepackage{mathptmx}
\usepackage{bookman}

  	 
\begin{document}

\section{Single Sample}
We define a sample $\mathcal{S}_i$ 
by a set of $N_i$ \emph{scalar abscissae} $X_{ij}$ with corresponding \emph{vectorial ordinates} $\vec{Y}_{ij}$ of $\mathbb{R}^d$.

We want to fit this sample by a \emph{vectorial function of scalar variable} $\vec{F}$ which depends on a set of parameters $\vec{a}\in\mathbb{R}^M$.

We define the distance over $\mathcal{S}_i$:
\begin{equation}
	D^2_i = \dfrac{1}{2}\sum_{j=1}^{N_i} \left[ \vec{Y}_{ij} - \vec{F}(X_{ij},\vec{a})\right]^2
\end{equation}

\begin{equation}
	D^2_i(\vec{a}+\delta\vec{a}) \simeq D^2_i(\vec{a}) + \langle \vec{\nabla}_{\vec{a}} D^2_i \vert \delta \vec{a} \rangle
	+ \dfrac{1}{2} \langle \delta\vec{a} \vert \mathcal{H}_i \vert \delta \vec{a} \rangle
\end{equation}
The minimum is around the solution of:
\begin{equation}
	\bm{\alpha}_i   \delta \vec{a}  \simeq -\vec{\nabla}_{\vec{a}} D^2_i = \vec{\beta}_i
\end{equation}

\begin{equation}
	 \partial_{a_k}D^2_i = -\sum_{j=1}^{N_i} \left[ \vec{Y}_{ij} - \vec{F}(X_{ij},\vec{a})\right] \cdot \partial_{a_k}\vec{F}(X_{ij},\vec{a})
\end{equation}

\begin{equation}
	\alpha_{ikl} = \partial_{a_l,a_k} D^2_i 
	= \sum_{j=1}^{N_i} \left(
	\partial_{a_k}\vec{F}(X_{ij},\vec{a}) \cdot \partial_{a_l}\vec{F}(X_{ij},\vec{a})
	- \left[ \vec{Y}_{ij} - \vec{F}(X_{ij},\vec{a})\right] \partial_{a_l,a_k}\cdot\vec{F}(X_{ij},\vec{a})  \right)
\end{equation}

\begin{equation}
	\beta_{ik} = \sum_{j=1}^{N_i} \left[ \vec{Y}_{ij} - \vec{F}(X_{ij},\vec{a})\right] \cdot \partial_{a_k}\vec{F}(X_{ij},\vec{a})
\end{equation}

\section{Multiple Samples}
Let us combine $S$ samples with natural respective weights $N_i$:
\begin{equation}
	 D^2(\vec{a}) = \left(\sum_{i=1}^S N_i D^2_i(\vec{a}) \right) /  \left(\sum_{i=1}^S N_i \right)
\end{equation}
We get:
\begin{equation}
	\vec{\beta} = 1/\left(\sum_{i=1}^S N_i \right) \left( \sum_{i=1}^S N_i \vec{\beta}_i\right)
\end{equation}
and
\begin{equation}
		 \bm{\alpha} = 1/\left(\sum_{i=1}^S N_i \right) \left( \sum_{i=1}^S N_i \bm{\alpha}_i\right)
\end{equation}

\section{Decreasing}

\begin{equation}
	 D^2(\vec{a} + \delta \vec{a}) \simeq 
	  D^2(\vec{a}) - \langle \vec{\beta} \vert \delta\vec{a} \rangle 
	  + \dfrac{1}{2} \langle \delta\vec{a} \vert \bm{\alpha}  \vert \delta\vec{a} \rangle 
\end{equation}


\begin{equation}
	\bm{\alpha}_\lambda =
	\left\lbrace
	\begin{array}{ccl}
	\underbrace{\alpha_{ii}}_{\geq0}(1+w_i\lambda) & \iff & i=j\\
	\alpha_{ij} & \iff & i \not= j \\
	\end{array}
	\right.
\end{equation}

The guess step is:
\begin{equation}
	\delta \vec{a}_\lambda = \bm{\alpha}_\lambda^{-1} \vec \beta
\end{equation}

\begin{equation}
	\delta \vec{a}_\lambda \to 
	\dfrac{1}{\lambda}
	\begin{bmatrix}
		\vdots\\
		\dfrac{ \beta_i }{w_i\alpha_{ii}}\\
		\vdots\\
	\end{bmatrix}
\end{equation}

\section{Errors}
For no error, we have $\vec{a}_0$ and $D^2(\vec{a}_0)=0$.
With error, we have $\vec{a}_0+\delta\vec{a}$ and 
\begin{equation}
D^2(\vec{a}_0+\delta\vec{a})=D^2(\vec{a}_0) - \vec{\beta}_0 \cdot \delta\vec{a} + \frac{1}{2} 
\langle \delta \vec{a} \vert \bm{\alpha}_0 \vert \delta \vec{a}\rangle
\end{equation}

\begin{equation}
	\gamma_i = \delta a_i \sqrt{\dfrac{\alpha_{ii}}{D^2}} \iff \delta \vec{a} = 
	\underbrace{
	\mathrm{diagm}
	\left(
	\begin{bmatrix}
	\vdots\\
	\sqrt{D^2/\alpha_{ii}}\\
	\vdots\\
	\end{bmatrix}
	\right)}_{\bm{\Delta}_w} \vec{\gamma}
\end{equation}

\begin{equation}
	2 D^2 = \langle \vec{\gamma} \vert \bm{\Delta}_w \bm{\alpha}_0 \bm{\Delta}_w \vert \vec{\gamma} \rangle
\end{equation}

\begin{equation}
	\bm{\Delta}_w \bm{\alpha}_0 \bm{\Delta}_w = \bm{P} \bm{\Delta}_\mu \bm{P}^t
\end{equation}

\begin{equation}
	\vec{\rho} = \bm{P}^t \vec{\gamma}
\end{equation}

\begin{equation}
	2D^2 = \langle \vec{\rho} \vert \bm{\Delta}_\mu \vert \vec{\rho} \rangle
\end{equation}

\begin{equation}
	2D^2 = \sum_i \mu_i \rho_i^2, \text{$\mu_i>0$ since minimum}
\end{equation}

\begin{equation}
	1 = \sum_i  \dfrac{\mu_i}{2D^2} \rho_i^2 = \sum_i \left(\dfrac{\rho_i}{ \sqrt{\dfrac{2D^2}{\mu_i}} }\right)^2
\end{equation}

We get $\vec{\rho}$ on an ellipsoid with radii $\sqrt{\dfrac{2D^2}{\mu_i}}$

\begin{equation}
	\delta\vec{a} = \bm{\Delta}_w\bm{P} \vec{\rho} \iff \vec{\rho} = \bm{P}^t \bm{\Delta}_w^{-1} \delta\vec{a}
\end{equation}

\begin{equation}
	L = \dfrac{1}{2} \delta\vec{a}^2 - \kappa (2D^2 - \langle \vec{\rho} \vert \bm{\Delta}_\mu \vert \vec{\rho} \rangle)
\end{equation}

\end{document}
